{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 12:11:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 79.0MB/s]                    \n",
      "2022-11-15 12:11:07 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2022-11-15 12:11:07 INFO: Use device: gpu\n",
      "2022-11-15 12:11:07 INFO: Loading: tokenize\n",
      "2022-11-15 12:11:07 INFO: Loading: pos\n",
      "2022-11-15 12:11:07 INFO: Loading: constituency\n",
      "2022-11-15 12:11:08 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "from collections import defaultdict\n",
    "from ast import parse\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from structured_stable_diffusion.util import instantiate_from_config\n",
    "from structured_stable_diffusion.models.diffusion.ddim import DDIMSampler\n",
    "from structured_stable_diffusion.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "\n",
    "import stanza\n",
    "from nltk.tree import Tree\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')\n",
    "import pdb\n",
    "import json\n",
    "import sng_parser\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "\n",
    "def preprocess_prompts(prompts):\n",
    "    if isinstance(prompts, (list, tuple)):\n",
    "        return [p.lower().strip().strip(\".\").strip() for p in prompts]\n",
    "    elif isinstance(prompts, str):\n",
    "        return prompts.lower().strip().strip(\".\").strip()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_all_nps(tree, full_sent=None):\n",
    "    start = 0\n",
    "    end = len(tree.leaves())\n",
    "\n",
    "    def get_sub_nps(tree, left, right):\n",
    "        if isinstance(tree, str) or len(tree.leaves()) == 1:\n",
    "            return []\n",
    "        sub_nps = []\n",
    "        n_leaves = len(tree.leaves())\n",
    "        n_subtree_leaves = [len(t.leaves()) for t in tree]\n",
    "        offset = np.cumsum([0] + n_subtree_leaves)[:len(n_subtree_leaves)]\n",
    "        assert right - left == n_leaves\n",
    "        if tree.label() == 'NP' and n_leaves > 1:\n",
    "            sub_nps.append([\" \".join(tree.leaves()), (int(left), int(right))])\n",
    "        for i, subtree in enumerate(tree):\n",
    "            sub_nps += get_sub_nps(subtree, left=left+offset[i], right=left+offset[i]+n_subtree_leaves[i])\n",
    "        return sub_nps\n",
    "    \n",
    "    all_nps = get_sub_nps(tree, left=start, right=end)\n",
    "    lowest_nps = []\n",
    "    for i in range(len(all_nps)):\n",
    "        span = all_nps[i][1]\n",
    "        lowest = True\n",
    "        for j in range(len(all_nps)):\n",
    "            span2 = all_nps[j][1]\n",
    "            if span2[0] >= span[0] and span2[1] <= span[1]:\n",
    "                lowest = False\n",
    "                break\n",
    "        if lowest:\n",
    "            lowest_nps.append(all_nps[i])\n",
    "\n",
    "    all_nps, spans = map(list, zip(*all_nps))\n",
    "    if full_sent and full_sent not in all_nps:\n",
    "        all_nps = [full_sent] + all_nps\n",
    "        spans = [(start, end)] + spans\n",
    "\n",
    "    return all_nps, spans, lowest_nps\n",
    "\n",
    "\n",
    "def get_all_spans_from_scene_graph(caption):\n",
    "    caption = caption.strip()\n",
    "    graph = sng_parser.parse(caption)\n",
    "    nps = []\n",
    "    spans = []\n",
    "    words = caption.split()\n",
    "    for e in graph['entities']:\n",
    "        start, end = e['span_bounds']\n",
    "        if e['span'] == caption: continue\n",
    "        if end-start == 1: continue\n",
    "        nps.append(e['span'])\n",
    "        spans.append(e['span_bounds'])\n",
    "    for r in graph['relations']:\n",
    "        start1, end1 = graph['entities'][r['subject']]['span_bounds']\n",
    "        start2, end2 = graph['entities'][r['object']]['span_bounds']\n",
    "        start = min(start1, start2)\n",
    "        end = max(end1, end2)\n",
    "        if \" \".join(words[start:end]) == caption: continue\n",
    "        nps.append(\" \".join(words[start:end]))\n",
    "        spans.append((start, end))\n",
    "    \n",
    "    return [caption] + nps, [(0, len(words))] + spans, None\n",
    "\n",
    "\n",
    "\n",
    "def expand_sequence(seq, length, dim=1):\n",
    "    seq = seq.transpose(0, dim)\n",
    "    max_length = seq.size(0)\n",
    "    n_repeat = (max_length - 2) // length\n",
    "    repeat_size = (n_repeat,) + (1, ) * (len(seq.size()) -1)\n",
    "\n",
    "    eos = seq[length+1, ...].clone()\n",
    "    segment = seq[1:length+1, ...].repeat(*repeat_size)\n",
    "    seq[1:len(segment)+1] = segment\n",
    "    seq[len(segment)+1] = eos\n",
    "\n",
    "    return seq.transpose(0, dim)\n",
    "\n",
    "\n",
    "def align_sequence(main_seq, seq, span, eos_loc, dim=1, zero_out=False, replace_pad=False):\n",
    "    seq = seq.transpose(0, dim)\n",
    "    main_seq = main_seq.transpose(0, dim)\n",
    "    start, end = span[0]+1, span[1]+1\n",
    "    seg_length = end - start\n",
    "\n",
    "    main_seq[start:end] = seq[1:1+seg_length]\n",
    "    if zero_out:\n",
    "        main_seq[1:start] = 0\n",
    "        main_seq[end:eos_loc] = 0\n",
    "\n",
    "    if replace_pad:\n",
    "        pad_length = len(main_seq) - eos_loc\n",
    "        main_seq[eos_loc:] = seq[1+seg_length:1+seg_length+pad_length]\n",
    "    \n",
    "\n",
    "    return main_seq.transpose(0, dim)\n",
    "\n",
    "\n",
    "def get_actions(tree, SHIFT = 0, REDUCE = 1, OPEN='(', CLOSE=')'):\n",
    "    #input tree in bracket form: ((A B) (C D))\n",
    "    #output action sequence: S S R S S R R\n",
    "    actions = []\n",
    "    tree = tree.strip()\n",
    "    i = 0\n",
    "    num_shift = 0\n",
    "    num_reduce = 0\n",
    "    left = 0\n",
    "    right = 0\n",
    "    while i < len(tree):\n",
    "        if tree[i] != ' ' and tree[i] != OPEN and tree[i] != CLOSE: #terminal\n",
    "            if tree[i-1] == OPEN or tree[i-1] == ' ':\n",
    "                actions.append(SHIFT)\n",
    "                num_shift += 1\n",
    "        elif tree[i] == CLOSE:\n",
    "            actions.append(REDUCE)\n",
    "            num_reduce += 1\n",
    "            right += 1\n",
    "        elif tree[i] == OPEN:\n",
    "            left += 1\n",
    "        i += 1\n",
    "    pdb.set_trace()\n",
    "    assert(num_shift == num_reduce + 1)\n",
    "    return actions\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "    \"\"\"\n",
    "    Convert a numpy image or a batch of images to a PIL image.\n",
    "    \"\"\"\n",
    "    if images.ndim == 3:\n",
    "        images = images[None, ...]\n",
    "    images = (images * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "    return pil_images\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_replacement(x):\n",
    "    try:\n",
    "        hwc = x.shape\n",
    "        y = Image.open(\"assets/rick.jpeg\").convert(\"RGB\").resize((hwc[1], hwc[0]))\n",
    "        y = (np.array(y)/255.0).astype(x.dtype)\n",
    "        assert y.shape == x.shape\n",
    "        return y\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "# generating images from pre-trained model\n",
    "def sampling(model, sampler, prompt, n_samples, scale=7.5, steps=50, conjunction=False):\n",
    "    H = W = 512\n",
    "    C = 4\n",
    "    f = 8\n",
    "    precision_scope = autocast\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                all_samples = list()\n",
    "                for n in trange(n_samples, desc=\"Sampling\"):\n",
    "                    for bid, p in enumerate(prompt):\n",
    "                        p = preprocess_prompts(p)\n",
    "\n",
    "                        uc = model.get_learned_conditioning([\"\"])\n",
    "                        if isinstance(p, tuple):\n",
    "                            p = list(p)\n",
    "                        c = model.get_learned_conditioning(p)\n",
    "                        \n",
    "                        doc = nlp(p[0])\n",
    "                        mytree = Tree.fromstring(str(doc.sentences[0].constituency))\n",
    "                        nps, spans, noun_chunk = get_all_nps(mytree, p[0])\n",
    "                        # nps, spans, noun_chunk = get_all_spans_from_scene_graph(prompts[0].split(\"\\t\")[0])\n",
    "\n",
    "                        nps_length = [len(ids)-2 for ids in model.cond_stage_model.tokenizer(nps).input_ids]\n",
    "                        nps = [[np]*len(p) for np in nps]\n",
    "                        \n",
    "                        c = [model.get_learned_conditioning(np) for np in nps]\n",
    "                        k_c = [c[0]] + [align_sequence(c[0].clone(), seq, span, nps_length[0]+1) for seq, span in zip(c[1:], spans[1:])]\n",
    "                        v_c = [c[0]] + [align_sequence(c[0].clone(), seq, span, nps_length[0]+1) for seq, span in zip(c[1:], spans[1:])]\n",
    "                        \n",
    "                        if not conjunction:\n",
    "                            c = {'k': k_c[:1], 'v': v_c}\n",
    "                        else:\n",
    "                            c = {'k': k_c, 'v': v_c[-1:]}\n",
    "\n",
    "                        shape = [C, H // f, W // f]\n",
    "                        samples_ddim, _ = sampler.sample(S=steps,\n",
    "                                                            conditioning=c,\n",
    "                                                            batch_size=1,\n",
    "                                                            shape=shape,\n",
    "                                                            verbose=False,\n",
    "                                                            unconditional_guidance_scale=scale,\n",
    "                                                            unconditional_conditioning=uc,\n",
    "                                                            eta=0.0,\n",
    "                                                            x_T=None,\n",
    "                                                            quiet=True)\n",
    "\n",
    "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "                        x_checked_image = x_samples_ddim\n",
    "\n",
    "                        x_checked_image_torch = torch.from_numpy(x_checked_image).permute(0, 3, 1, 2)\n",
    "\n",
    "                        all_samples.append(x_checked_image_torch)\n",
    "    return all_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as TS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "grounding_dino_path = os.path.join(parent_dir, 'GroundingDINO')\n",
    "tag2text_path = os.path.join(parent_dir, 'Tag2Text')\n",
    "fastchat_path = os.path.join(parent_dir, 'fastchat')\n",
    "\n",
    "sys.path.insert(0, grounding_dino_path)\n",
    "sys.path.insert(0, tag2text_path)\n",
    "sys.path.insert(0, fastchat_path)\n",
    "print(f\"Updated sys.path: {sys.path}\")\n",
    "\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "\n",
    "\n",
    "# from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "from Tag2Text.models import tag2text\n",
    "from Tag2Text import inference\n",
    "\n",
    "config_file = os.path.join(parent_dir, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "tag2text_checkpoint = os.path.join(parent_dir, 'weight/tag2text_swin_14m.pth')\n",
    "grounded_checkpoint = os.path.join(parent_dir, 'weight/groundingdino_swint_ogc.pth')\n",
    "sam_checkpoint = os.path.join(parent_dir, 'weight/sam_vit_h_4b8939.pth')\n",
    "\n",
    "\n",
    "assert os.path.isfile(tag2text_checkpoint), f\"Checkpoint file {tag2text_checkpoint} not found.\"\n",
    "assert os.path.isfile(grounded_checkpoint), f\"Checkpoint file {grounded_checkpoint} not found.\"\n",
    "assert os.path.isfile(sam_checkpoint), f\"Checkpoint file {sam_checkpoint} not found.\"\n",
    "\n",
    "normalize = TS.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "transform = TS.Compose([\n",
    "    TS.Resize((384, 384)),\n",
    "    TS.ToTensor(), normalize\n",
    "])\n",
    "\n",
    "delete_tag_index = [i for i in range(3012, 3429)]\n",
    "tag2text_model = tag2text.tag2text_caption(pretrained=tag2text_checkpoint,\n",
    "                                           image_size=384,\n",
    "                                           vit='swin_b',\n",
    "                                           delete_tag_index=delete_tag_index)\n",
    "tag2text_model.threshold = 0.64\n",
    "tag2text_model.eval()\n",
    "tag2text_model = tag2text_model.to(device)\n",
    "\n",
    "specified_tags = 'None'\n",
    "\n",
    "from fastchat.serve.inference import ChatIO, generate_stream, load_model\n",
    "from fastchat.conversation import get_default_conv_template\n",
    "\n",
    "class SimpleChatIO(ChatIO):\n",
    "    def prompt_for_input(self, role, prompt, tags) -> str:\n",
    "        return f\"Please assess the described scene based on the provided prompt and determine the likelihood of each tag appearing in the scene. Assign a score to each tag according to the following criteria:  If a tag is certain to appear, assign a score of 3. If a tag may appear, assign a score of 2. If a tag is unlikely to appear, assign a score of 1.\\nPrompt: {prompt}; Tags: {tags}\"\n",
    "\n",
    "    def prompt_for_output(self, role: str):\n",
    "        print(f\"{role}: \", end=\"\", flush=True)\n",
    "\n",
    "    def stream_output(self, output_stream):\n",
    "        pre = 0\n",
    "        output = ''\n",
    "        for outputs in output_stream:\n",
    "            outputs = outputs.strip().split(\" \")\n",
    "            now = len(outputs) - 1\n",
    "            if now > pre:\n",
    "                output += \" \".join(outputs[pre:now])\n",
    "                pre = now\n",
    "        output += \" \".join(outputs[pre:])\n",
    "        return output\n",
    "\n",
    "chatio = SimpleChatIO()\n",
    "vicuna_path = \"lmsys/vicuna-7b-v1.5\"\n",
    "vicuna, tokenizer = load_model(vicuna_path, \"cuda\", 1, None, False, False, False)\n",
    "conv = get_default_conv_template(vicuna_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as TS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import clip\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "data_path = os.path.join(parent_dir, 'data/coco_train.npy')\n",
    "\n",
    "assert os.path.isfile(data_path), f\"Data file {data_path} not found.\"\n",
    "\n",
    "# load dataset\n",
    "coco_train = np.load(data_path, allow_pickle=True).tolist()\n",
    "length = len(coco_train)\n",
    "ftprompts = pd.DataFrame({'prompts': coco_train[:length]})\n",
    "\n",
    "class MSCOCODataset():\n",
    "    def __init__(self):\n",
    "        global ftprompts\n",
    "        self.ftprompts = ftprompts.iloc[:, 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ftprompts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        prompt = self.ftprompts.iloc[index]\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = str(prompt)\n",
    "\n",
    "        prompt = prompt.strip()\n",
    "        return prompt\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "dataset = MSCOCODataset()\n",
    "finetune_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# load CLIP model\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "# params = torch.load(\"../hpc.pt\")['state_dict']\n",
    "# clip_model.load_state_dict(params)\n",
    "\n",
    "# load blip-2model\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
    "caption = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
    "caption.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sam_score(prompt, tags, length):\n",
    "    inp = chatio.prompt_for_input(conv.roles[0], prompt, tags)\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    generate_stream_func = generate_stream\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    gen_params = {\n",
    "        \"model\": vicuna_path,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"stop\": conv.stop_str,\n",
    "        \"stop_token_ids\": conv.stop_token_ids,\n",
    "        \"echo\": False,\n",
    "    }\n",
    "\n",
    "    output_stream = generate_stream_func(vicuna, tokenizer, gen_params, device)\n",
    "    outputs = chatio.stream_output(output_stream)\n",
    "    sam_score = 0\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i].isdigit():\n",
    "            sam_score += int(outputs[i])\n",
    "    return (sam_score - 2 * length) / (2 * length)\n",
    "\n",
    "\n",
    "def get_tags(image_pil):\n",
    "    raw_image = image_pil.resize((384, 384))\n",
    "    raw_image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    res = inference.inference(raw_image, tag2text_model, specified_tags)\n",
    "    text_prompt = res[0].replace(' |', ',')\n",
    "    length = len(text_prompt.split(','))\n",
    "    return text_prompt, length\n",
    "\n",
    "\n",
    "def get_image_score(image, prompt):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize([prompt]).to(device)\n",
    "        text_features = clip_model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features.to(torch.float16)\n",
    "\n",
    "        tags, length = get_tags(image)\n",
    "        sam_score = get_sam_score(prompt, tags, length)\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = caption.generate(**inputs)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        captext = clip.tokenize([generated_text]).to(device)\n",
    "        caption_features = clip_model.encode_text(captext)\n",
    "        caption_features /= caption_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        cos_sim = torch.cosine_similarity(text_features, caption_features, dim=1)\n",
    "        reward = float(cos_sim) + float(sam_score)\n",
    "        return reward\n",
    "\n",
    "\n",
    "def get_cap_reward(image, prompt):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize([prompt]).to(device)\n",
    "        text_features = clip_model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features.to(torch.float16)\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = caption.generate(**inputs)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        captext = clip.tokenize([generated_text]).to(device)\n",
    "        caption_features = clip_model.encode_text(captext)\n",
    "        caption_features /= caption_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        cos_sim = torch.cosine_similarity(text_features, caption_features, dim=1)\n",
    "        reward = float(cos_sim)\n",
    "        return reward\n",
    "\n",
    "\n",
    "def get_sam_reward(image, prompt):\n",
    "    with torch.no_grad():\n",
    "        tags, length = get_tags(image)\n",
    "        sam_score = get_sam_score(prompt, tags, length)\n",
    "        reward = float(sam_score)\n",
    "        return reward\n",
    "\n",
    "\n",
    "def get_max_score(prompt_list, image_list, index, epoch=0, ARL=True):\n",
    "    if ARL:\n",
    "        cap_score_list = []\n",
    "        sam_score_list = []\n",
    "\n",
    "        print(f\"Length of image_list: {len(image_list)}\")\n",
    "        print(f\"Length of prompt_list: {len(prompt_list)}\")\n",
    "\n",
    "        # Iterate through each prompt and its corresponding set of images\n",
    "        for prompt_idx, (prompt, img_set) in enumerate(zip(prompt_list, image_list)):\n",
    "            print(f\"Processing image set {prompt_idx}, length of image set {len(img_set)}\")\n",
    "            # Iterate through all generated images for the current prompt\n",
    "            for img_idx, image in enumerate(img_set):\n",
    "                # Compute individual rewards\n",
    "                cap_score = get_cap_reward(image, prompt)\n",
    "                sam_score = get_sam_reward(image, prompt)\n",
    "\n",
    "                # Store the score along with its prompt/image indices\n",
    "                cap_score_list.append((cap_score, prompt_idx, img_idx))\n",
    "                sam_score_list.append((sam_score, prompt_idx, img_idx))\n",
    "\n",
    "        # Sort by descending scores; higher scores are better\n",
    "        cap_rankings = sorted(range(len(cap_score_list)), key=lambda x: cap_score_list[x][0], reverse=True)\n",
    "        sam_rankings = sorted(range(len(sam_score_list)), key=lambda x: sam_score_list[x][0], reverse=True)\n",
    "\n",
    "        # Rank by the sum of individual rankings\n",
    "        total_rankings = [\n",
    "            (cap_rankings.index(i) + sam_rankings.index(i), cap_score_list[i][1], cap_score_list[i][2])\n",
    "            for i in range(len(cap_score_list))\n",
    "        ]\n",
    "\n",
    "        # Select the item with the best (lowest) combined rank\n",
    "        best_ranking = min(total_rankings, key=lambda x: x[0])\n",
    "        best_total_rank = best_ranking[0]\n",
    "        best_prompt_idx = best_ranking[1]\n",
    "        best_img_idx = best_ranking[2]\n",
    "\n",
    "        # Optionally retrieve the best score\n",
    "        best_cap_score = cap_score_list[cap_rankings[best_prompt_idx]][0]\n",
    "        best_sam_score = sam_score_list[sam_rankings[best_prompt_idx]][0]\n",
    "        best_score = best_cap_score + best_sam_score\n",
    "\n",
    "        ftprompts.loc[index, f'Epoch{epoch} Scores'] = best_score\n",
    "\n",
    "        return [best_score, best_prompt_idx, best_img_idx]\n",
    "    else:\n",
    "        score_list = []\n",
    "        for i in range(len(prompt_list)):\n",
    "            score_list.append(get_image_score(image_list[i], prompt_list[i]))\n",
    "        torch.cuda.empty_cache()\n",
    "        ftprompts.loc[index, f'Epoch{epoch} Scores'] = max(score_list)\n",
    "        return [max(score_list), score_list.index(max(score_list))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@title Settings for the model\n",
    "\n",
    "#@markdown All settings have been configured to achieve optimal outputorch. Changing them is not advisable.\n",
    "\n",
    "#@markdown Enter value for `resolution`.\n",
    "resolution=512 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown Enter value for `num_images_per_prompt`.\n",
    "num_images_per_prompt=10 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown Enter value for `epochs`.\n",
    "epochs=2 #@param {type:\"integer\"} |\n",
    "\n",
    "#@markdown Enter value for `seed`.\n",
    "#generator = torch.Generator(device=device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models/ldm/stable-diffusion-v1/sd-v1-4.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckpt = \"../models/ldm/stable-diffusion-v1/model.ckpt\"\n",
    "\n",
    "config = OmegaConf.load(\"../configs/stable-diffusion/v1-inference.yaml\")\n",
    "model = load_model_from_config(config, f\"{ckpt}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "sampler = PLMSSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import concurrent\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "\n",
    "#os.environ['MODEL_NAME'] = model_id\n",
    "os.environ['OUTPUT_DIR'] = f\"./CustomModel/\"\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "topk = length\n",
    "training_steps_per_epoch = topk * 10\n",
    "os.environ['CHECKPOINTING_STEPS'] = str(training_steps_per_epoch)\n",
    "os.environ['RESOLUTION'] = str(resolution)\n",
    "os.environ['LEARNING_RATE'] = str(9e-6)\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./CustomModel')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree('./trainingdataset/imagefolder/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "total = 0\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    # training step\n",
    "    training_steps = str(training_steps_per_epoch * (epoch + 1))\n",
    "    os.environ['TRAINING_STEPS'] = training_steps\n",
    "    os.environ['TRAINING_DIR'] = f'./trainingdataset/imagefolder/{epoch}'\n",
    "\n",
    "    training_prompts = []\n",
    "\n",
    "    ftprompts[f'Epoch{epoch} Scores'] = np.nan\n",
    "\n",
    "    for step, prompt_list in enumerate(finetune_dataloader):\n",
    "        print(prompt_list)\n",
    "\n",
    "        image_list = []\n",
    "        for prompt in prompt_list:\n",
    "            print(prompt)\n",
    "            all_samples = sampling(model, sampler, [[prompt]], num_images_per_prompt, scale=7.5, steps=50)\n",
    "\n",
    "\n",
    "            images = []\n",
    "\n",
    "            for sample in all_samples:\n",
    "                for img in sample:\n",
    "                    img = img * 255.0\n",
    "                    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "                    img = Image.fromarray(img.astype(np.uint8))\n",
    "\n",
    "                    images.append(img)\n",
    "\n",
    "\n",
    "            image_list.append(images)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "        step_list = [i for i in range(step * batch_size, (step + 1) * batch_size)]\n",
    "        for idx in step_list:\n",
    "            best_score, best_prompt_idx, best_img_idx = get_max_score(prompt_list, image_list, idx, epoch)\n",
    "            print(f\"best_score: {best_score}, best_prompt_idx: {best_prompt_idx}, best_img_idx: {best_img_idx}\")\n",
    "\n",
    "            best_image = image_list[best_prompt_idx][best_img_idx]\n",
    "            best_prompt = prompt_list[best_prompt_idx]\n",
    "\n",
    "            training_prompts.append([best_score, best_image, best_prompt])\n",
    "\n",
    "\n",
    "\n",
    "    training_prompts = [row[1:3] for row in sorted(training_prompts, key=lambda x: x[0], reverse=True)[:topk]]\n",
    "    training_prompts = pd.DataFrame(training_prompts)\n",
    "\n",
    "    if not os.path.exists(f\"./trainingdataset/imagefolder/{epoch}/train/\"):\n",
    "        os.makedirs(f\"./trainingdataset/imagefolder/{epoch}/train/\")\n",
    "    if not os.path.exists(f\"./CustomModel/\"):\n",
    "        os.makedirs(f\"./CustomModel/\")\n",
    "\n",
    "    for i in range(len(training_prompts)):\n",
    "        training_prompts.iloc[i, 0].save(f'./trainingdataset/imagefolder/{epoch}/train/{i}.png')\n",
    "\n",
    "    training_prompts['file_name'] = [f\"{i}.png\" for i in range(len(training_prompts))]\n",
    "    training_prompts.columns = ['image', 'text', 'file_name']\n",
    "    training_prompts.drop('image', axis=1, inplace=True)\n",
    "    training_prompts.to_csv(f'./trainingdataset/imagefolder/{epoch}/train/metadata.csv', index=False)\n",
    "\n",
    "    # start training\n",
    "    if epoch < epochs:\n",
    "        !accelerate launch --num_processes=1 --mixed_precision='fp16' --dynamo_backend='no' --num_machines=1 train_lora.py \\\n",
    "            --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "            --train_data_dir=$TRAINING_DIR \\\n",
    "            --resolution=$RESOLUTION \\\n",
    "            --train_batch_size=8 \\\n",
    "            --gradient_accumulation_steps=1 \\\n",
    "            --gradient_checkpointing \\\n",
    "            --max_grad_norm=1 \\\n",
    "            --mixed_precision=\"fp16\" \\\n",
    "            --max_train_steps=$TRAINING_STEPS \\\n",
    "            --learning_rate=$LEARNING_RATE \\\n",
    "            --lr_warmup_steps=0 \\\n",
    "            --enable_xformers_memory_efficient_attention \\\n",
    "            --dataloader_num_workers=5 \\\n",
    "            --output_dir=$OUTPUT_DIR \\\n",
    "            --lr_warmup_steps=0 \\\n",
    "            --seed=1234 \\\n",
    "            --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "            --resume_from_checkpoint=\"latest\" \\\n",
    "            --lr_scheduler='constant'\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('stable_diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbb0e8168c651416c036ae65c41812ab6dad5d8ae12f8c7b224865af9cc3551d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
